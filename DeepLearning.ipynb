{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Read CSV, place in DF\n",
    "df = pd.read_csv(\"features.csv\")\n",
    "df.head()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heartWillGoOn = [{'acousticness': 0.732, 'danceability': 0.428, 'energy': 0.276,  \n",
    "                 'id': '3oEHQmhvFLiE7ZYES0ulzv', 'instrumentalness': 5.33e-06, 'key': 4, \n",
    "                 'liveness': 0.117,'loudness': -11.729,'mode': 1,\n",
    "                  'speechiness': 0.0312, 'tempo': 99.195, 'valence': 0.0382}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(heartWillGoOn)\n",
    "df = df.reset_index()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df[['acousticness','danceability','energy','instrumentalness','key',\\\n",
    "                                           'liveness','loudness','mode','speechiness','tempo','valence']])\n",
    "df_normalized = pd.DataFrame(np_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.columns = ['acousticness','danceability','energy','instrumentalness','key',\\\n",
    "                                           'liveness','loudness','mode','speechiness','tempo','valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['id'] = df['id']\n",
    "df_normalized.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized,last_row=df_normalized.drop(df_normalized.tail(1).index),df_normalized.tail(1)\n",
    "last_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row['acousticness']\n",
    "df_normalized['score'] = ( \n",
    "    df_normalized['acousticness'].apply(lambda row: np.abs(row - last_row['acousticness'])) + \\\n",
    "    df_normalized['danceability'].apply(lambda row: np.abs(row - last_row['danceability'])) + \\\n",
    "    df_normalized['energy'].apply(lambda row: np.abs(row - last_row['energy'])) + \\\n",
    "    df_normalized['instrumentalness'].apply(lambda row: np.abs(row - last_row['instrumentalness'])) + \\\n",
    "    df_normalized['key'].apply(lambda row: np.abs(row - last_row['key'])) + \\\n",
    "    df_normalized['liveness'].apply(lambda row: np.abs(row - last_row['liveness'])) + \\\n",
    "    df_normalized['loudness'].apply(lambda row: np.abs(row - last_row['loudness'])) + \\\n",
    "    df_normalized['mode'].apply(lambda row: np.abs(row - last_row['mode'])) + \\\n",
    "    df_normalized['speechiness'].apply(lambda row: np.abs(row - last_row['speechiness'])) + \\\n",
    "    df_normalized['tempo'].apply(lambda row: np.abs(row - last_row['tempo'])) + \\\n",
    "    df_normalized['valence'].apply(lambda row: np.abs(row - last_row['valence']))                                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['class'] = df_normalized['score'].apply(lambda row: True if row < 1.5 else False)\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86a9e3212f200d21",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = df_normalized.drop([\"class\", \"score\",\"id\"], axis=1)\n",
    "#feature_names = data.columns\n",
    "data.head()\n",
    "target = df_normalized[\"class\"]\n",
    "print(data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-711a82d9b32c83ff",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, random_state=20, stratify=target)\n",
    "# X_scaler = StandardScaler().fit(X_train)\n",
    "# X_train_scaled = X_scaler.transform(X_train)\n",
    "# X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='selu', input_dim=11))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "#     X_train_scaled,  Test 2 - Removed scaling.  This step was performed on data BEFORE being entered into machine\n",
    "    X_train,\n",
    "    y_train_categorical,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For second test, sub'd X_test_scaled for _test\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test, y_test_categorical, verbose=2) \n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f97eb3e97245187b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#For second test, sub'd X_test_scaled for _test\n",
    "encoded_predictions = model.predict_classes(X_test[:100])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {list(y_test[:100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Provide Name for Saved File: \")\n",
    "# Save the model\n",
    "folder = \"Saved Models\"\n",
    "model.save(f'{folder}\\{name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(df_normalized[['acousticness','danceability','energy','instrumentalness','key',\\\n",
    "                                           'liveness','loudness','mode','speechiness','tempo','valence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the probabilities of NOT RECOMMENDED we can choose a random 3 songs which have a probability of 1 to find a bad song\n",
    "not_rec_list = []\n",
    "for index, item in enumerate(probs):\n",
    "#     print(index, item)\n",
    "    if item[0] == 1:\n",
    "        not_rec_list.append({\"id\":index,\n",
    "                         \"score\": item[0]})\n",
    "        \n",
    "not_rec_sel = random.sample(not_rec_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probabilities of recommendation (not NOT recommended) went far lower than expected\n",
    "rec_list = []\n",
    "for index, item in enumerate(probs):\n",
    "#     print(index, item)\n",
    "    if item[0] < 0.000001:\n",
    "        rec_list.append({\"id\":index,\n",
    "                         \"score\": item[0]})\n",
    "        \n",
    "rec_sel = random.sample(rec_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the lowest score gives us the single highest recommended song\n",
    "bestScoreItem = min(rec_list, key=lambda x:x['score'])\n",
    "bestScoreItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the song id's of our not recommended songs\n",
    "for i in not_rec_sel:\n",
    "    index = i[\"id\"]\n",
    "    print(df_normalized.loc[index][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the song id of our highest recommendation\n",
    "index = bestScoreItem[\"id\"]\n",
    "print(df_normalized.loc[index][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A different approach was needed to get our top three songs\n",
    "newlist = sorted(rec_list, key=lambda k: k['score']) \n",
    "newlist[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the song ids of our top three songs (note the one duplicate)\n",
    "for i in newlist[0:4]:\n",
    "    index = i[\"id\"]\n",
    "    print(df_normalized.loc[index][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
